#!/usr/bin/env python3
"""
ExplicaciÃ³n: Â¿CÃ³mo captura el sistema la INTENCIÃ“N del usuario?

Respuesta corta: NO la mide explÃ­citamente. Es una propiedad EMERGENTE
del entrenamiento del modelo de embeddings en millones de textos.
"""

import numpy as np
from fastembed import TextEmbedding

print("="*80)
print("Â¿CÃ³mo Captura el Sistema la INTENCIÃ“N del Usuario?")
print("="*80)

# Cargar el modelo de embeddings
model = TextEmbedding(model_name="BAAI/bge-small-en-v1.5")

# ============================================================================
# PARTE 1: El modelo NO tiene lÃ³gica explÃ­cita de "intenciÃ³n"
# ============================================================================
print("\n" + "="*80)
print("PARTE 1: El Modelo NO Tiene LÃ³gica ExplÃ­cita de 'IntenciÃ³n'")
print("="*80)

print("""
âŒ El modelo NO hace esto:
   if "hungry" in query and "NOW" in query:
       intention = "quick_meal"
   
âŒ NO tiene reglas if/else para detectar intenciones

âœ… En cambio, el modelo fue ENTRENADO en millones de textos donde:
   â€¢ Vio "I'm hungry" junto a "quick", "fast", "easy"
   â€¢ Vio "impress guests" junto a "elegant", "sophisticated", "special"
   â€¢ Vio "comfort food" junto a "warm", "hearty", "cozy"
   
   Y aprendiÃ³ a colocar estos conceptos CERCA en el espacio vectorial.
""")

# ============================================================================
# PARTE 2: DemostraciÃ³n - Queries con misma INTENCIÃ“N = Vectores similares
# ============================================================================
print("\n" + "="*80)
print("PARTE 2: Queries con Misma INTENCIÃ“N â†’ Vectores Similares")
print("="*80)

# Grupo 1: IntenciÃ³n = "Quiero algo rÃ¡pido"
quick_queries = [
    "I'm hungry and need something NOW",
    "quick recipe",
    "fast meal",
    "what can I make in 10 minutes?",
    "I don't have much time",
]

# Grupo 2: IntenciÃ³n = "Quiero impresionar"
impress_queries = [
    "I want to impress my dinner guests",
    "elegant recipe for special occasion",
    "sophisticated dish",
    "something fancy for a date",
    "gourmet meal",
]

def get_embedding(text):
    return list(model.embed([text]))[0]

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print("\nğŸ” Grupo 1: IntenciÃ³n 'RÃPIDO/URGENTE'\n")
quick_embeddings = [get_embedding(q) for q in quick_queries]

print("Similitud entre queries del MISMO grupo (misma intenciÃ³n):\n")
for i, q1 in enumerate(quick_queries[:3]):
    for j, q2 in enumerate(quick_queries[i+1:4], i+1):
        sim = cosine_similarity(quick_embeddings[i], quick_embeddings[j])
        print(f"  '{q1[:40]}...'")
        print(f"  '{q2[:40]}...'")
        print(f"  â†’ Similitud: {sim:.3f}\n")

print("\nğŸ” Grupo 2: IntenciÃ³n 'IMPRESIONAR/SOFISTICADO'\n")
impress_embeddings = [get_embedding(q) for q in impress_queries]

print("Similitud entre queries del MISMO grupo (misma intenciÃ³n):\n")
for i, q1 in enumerate(impress_queries[:3]):
    for j, q2 in enumerate(impress_queries[i+1:4], i+1):
        sim = cosine_similarity(impress_embeddings[i], impress_embeddings[j])
        print(f"  '{q1[:40]}...'")
        print(f"  '{q2[:40]}...'")
        print(f"  â†’ Similitud: {sim:.3f}\n")

print("\nğŸ“Š ComparaciÃ³n ENTRE grupos (intenciones diferentes):\n")
sim_cross = cosine_similarity(quick_embeddings[0], impress_embeddings[0])
print(f"  '{quick_queries[0]}'")
print(f"  vs")
print(f"  '{impress_queries[0]}'")
print(f"  â†’ Similitud: {sim_cross:.3f}")
print(f"\n  ğŸ’¡ Similitud MENOR porque tienen INTENCIONES diferentes")

# ============================================================================
# PARTE 3: Â¿CÃ³mo aprende el modelo estas relaciones?
# ============================================================================
print("\n" + "="*80)
print("PARTE 3: Â¿CÃ³mo AprendiÃ³ el Modelo Estas Relaciones?")
print("="*80)

print("""
ğŸ§  ENTRENAMIENTO DEL MODELO (BAAI/bge-small-en-v1.5):

1ï¸âƒ£  CORPUS DE ENTRENAMIENTO (millones de documentos):
   â€¢ ArtÃ­culos de cocina: "quick dinner recipes for busy weeknights"
   â€¢ ReseÃ±as: "I was hungry and needed something fast"
   â€¢ Blogs: "impress your guests with this elegant dish"
   â€¢ Recetas: "sophisticated gourmet meal for special occasions"

2ï¸âƒ£  OBJETIVO DEL ENTRENAMIENTO:
   â€¢ Textos que aparecen en CONTEXTOS SIMILARES â†’ vectores cercanos
   â€¢ Textos que aparecen en CONTEXTOS DIFERENTES â†’ vectores lejanos
   
   Ejemplo de contexto:
   - "I'm [MASK]" â†’ "hungry", "starving", "famished" (similares)
   - "Quick [MASK]" â†’ "recipe", "meal", "dish" (similares)

3ï¸âƒ£  RESULTADO:
   El modelo aprendiÃ³ que:
   â€¢ "hungry" + "NOW" + "quick" â†’ comparten contextos similares
   â€¢ "impress" + "guests" + "elegant" â†’ comparten contextos similares
   â€¢ Estos dos grupos NO comparten contextos â†’ vectores lejanos

4ï¸âƒ£  NO HAY LÃ“GICA EXPLÃCITA:
   âŒ No hay reglas: if "hungry" then intention="quick"
   âœ… Solo matemÃ¡ticas: vectores cercanos = significado similar
   
ğŸ’¡ La "intenciÃ³n" emerge naturalmente de patrones aprendidos en millones de textos.
""")

# ============================================================================
# PARTE 4: VisualizaciÃ³n del Espacio Vectorial (Simplificado)
# ============================================================================
print("\n" + "="*80)
print("PARTE 4: VisualizaciÃ³n del Espacio Vectorial (384D â†’ 2D)")
print("="*80)

print("""
ğŸ“ Espacio Vectorial Real: 384 dimensiones
   Imposible de visualizar directamente

ğŸ“Š ProyecciÃ³n simplificada a 2D (solo para ilustrar):
   
   "impress guests" â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— "elegant meal"
                    â”‚                         â”‚
                    â”‚  RegiÃ³n: SOFISTICADO   â”‚
                    â”‚                         â”‚
                    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
                    
                    
                    
   "quick recipe"   â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— "fast meal"
                    â”‚                         â”‚
                    â”‚  RegiÃ³n: RÃPIDO        â”‚
                    â”‚                         â”‚
   "I'm hungry NOW" â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
   
   
ğŸ’¡ En el espacio real de 384D:
   â€¢ Queries con misma intenciÃ³n estÃ¡n CERCA (distancia coseno pequeÃ±a)
   â€¢ Queries con intenciones diferentes estÃ¡n LEJOS
   â€¢ La bÃºsqueda encuentra el vector mÃ¡s CERCANO al query
""")

# ============================================================================
# PARTE 5: Â¿QuÃ© pasa en la bÃºsqueda?
# ============================================================================
print("\n" + "="*80)
print("PARTE 5: Â¿QuÃ© Pasa Cuando Haces una BÃºsqueda?")
print("="*80)

print("""
PASO A PASO:

1ï¸âƒ£  Usuario escribe: "I'm hungry and need something NOW"
    â†“
    
2ï¸âƒ£  Sistema genera embedding (vector de 384 nÃºmeros):
    query_vector = [-0.109, -0.018, 0.017, ..., 0.003]
    â†“
    
3ï¸âƒ£  Sistema calcula distancia coseno con TODOS los chunks en la BD:
    
    "TOMATO & MUSSEL PASTA - Quick 20min recipe"
    recipe_vector = [-0.105, -0.021, 0.019, ..., 0.005]
    similarity = cosine_distance(query_vector, recipe_vector)
    â†’ 0.703 (Â¡cercano!)
    
    "Christmas Pudding - Traditional 3-hour recipe"
    recipe_vector = [0.089, 0.112, -0.034, ..., -0.022]
    similarity = cosine_distance(query_vector, recipe_vector)
    â†’ 0.412 (lejano)
    
    â†“
    
4ï¸âƒ£  Sistema ordena por similitud (mÃ¡s cercano primero):
    1. TOMATO & MUSSEL PASTA (0.703) â† IntenciÃ³n: rÃ¡pido
    2. Quick Fish Pie (0.655) â† IntenciÃ³n: rÃ¡pido
    3. ... otras recetas rÃ¡pidas
    
    â†“
    
5ï¸âƒ£  Usuario recibe recetas que coinciden con su INTENCIÃ“N
    (aunque no usÃ³ palabras exactas como "quick" o "fast")

ğŸ’¡ El sistema NO "detecta" intenciÃ³n explÃ­citamente.
   Solo encuentra vectores CERCANOS, y estos vectores estÃ¡n cercanos
   porque el modelo aprendiÃ³ esos patrones en el entrenamiento.
""")

# ============================================================================
# PARTE 6: ComparaciÃ³n con Query ExplÃ­cito
# ============================================================================
print("\n" + "="*80)
print("PARTE 6: ComparaciÃ³n - ExplÃ­cito vs ImplÃ­cito")
print("="*80)

queries_comparison = [
    ("quick pasta", "Query EXPLÃCITO (tiene keyword)"),
    ("I need something fast", "IntenciÃ³n IMPLÃCITA (sin keyword 'quick')"),
    ("I'm starving", "IntenciÃ³n IMPLÃCITA (sin keywords de velocidad)"),
]

print("\nComparando embeddings:\n")
embeddings_comp = [get_embedding(q[0]) for q in queries_comparison]

for i, (q1, desc1) in enumerate(queries_comparison):
    for j, (q2, desc2) in enumerate(queries_comparison[i+1:], i+1):
        sim = cosine_similarity(embeddings_comp[i], embeddings_comp[j])
        print(f"  '{q1}' ({desc1})")
        print(f"  vs")
        print(f"  '{q2}' ({desc2})")
        print(f"  â†’ Similitud: {sim:.3f}")
        print()

print("""
ğŸ’¡ Observa:
   â€¢ "quick pasta" y "I need something fast" â†’ similitud ALTA
   â€¢ Aunque NO usan las mismas palabras
   â€¢ El modelo aprendiÃ³ que ambos expresan la misma INTENCIÃ“N
   â€¢ Sin lÃ³gica explÃ­cita, solo matemÃ¡ticas de vectores
""")

# ============================================================================
# RESUMEN FINAL
# ============================================================================
print("\n" + "="*80)
print("ğŸ“ RESUMEN: Â¿CÃ³mo Captura IntenciÃ³n Sin Medirla?")
print("="*80)

print("""
âœ… NO HAY LÃ“GICA DE "DETECCIÃ“N DE INTENCIÃ“N":
   â€¢ Sin reglas if/else
   â€¢ Sin clasificadores de intenciÃ³n
   â€¢ Sin anÃ¡lisis sintÃ¡ctico

âœ… ES GEOMETRÃA EN ESPACIO VECTORIAL:
   â€¢ Cada texto â†’ vector de 384 nÃºmeros
   â€¢ Textos similares en SIGNIFICADO â†’ vectores CERCANOS
   â€¢ Similitud = distancia coseno entre vectores
   
âœ… APRENDIDO DURANTE ENTRENAMIENTO:
   â€¢ Modelo vio millones de textos
   â€¢ AprendiÃ³ que "hungry NOW" y "quick recipe" aparecen en contextos similares
   â€¢ Los colocÃ³ cerca en el espacio vectorial
   â€¢ La "intenciÃ³n" emerge de estos patrones

âœ… EN LA BÃšSQUEDA:
   1. Query â†’ vector
   2. Calcular distancia a todos los chunks
   3. Ordenar por cercanÃ­a
   4. Retornar los mÃ¡s cercanos
   
   Â¡Eso es todo! Sin magia, solo Ã¡lgebra lineal.

ğŸ¯ ANALOGÃA:
   Es como el GPS: no "entiende" quÃ© es una ciudad,
   pero sabe que ParÃ­s y Lyon estÃ¡n cerca en el mapa (espacio 2D).
   
   Embeddings: "I'm hungry NOW" y "quick recipe" estÃ¡n cerca
   en el mapa semÃ¡ntico (espacio 384D).

ğŸš€ POR ESO FUNCIONA TAN BIEN:
   â€¢ No necesitas pensar en todas las posibles formas de expresar una intenciÃ³n
   â€¢ El modelo YA aprendiÃ³ esas relaciones de millones de textos
   â€¢ Solo busca "vecinos cercanos" en el espacio vectorial
""")

print("\n" + "="*80)
print("âœ… ExplicaciÃ³n completada!")
print("="*80)

