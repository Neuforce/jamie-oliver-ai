#!/usr/bin/env python3
"""
ExplicaciÃ³n: Â¿CÃ³mo un texto se convierte en un vector de 384 dimensiones?

Vamos a desglosar TODO el proceso, desde las palabras hasta los nÃºmeros.
"""

import numpy as np
from fastembed import TextEmbedding

print("="*80)
print("Â¿CÃ³mo un TEXTO se convierte en un VECTOR de 384 dimensiones?")
print("="*80)

# ============================================================================
# PASO 1: El Input (texto crudo)
# ============================================================================
print("\n" + "="*80)
print("PASO 1: El Input - Texto Crudo")
print("="*80)

text = "quick pasta recipe"
print(f"\nTexto original: '{text}'")
print(f"Tipo: {type(text)}")
print(f"Longitud: {len(text)} caracteres")

# ============================================================================
# PASO 2: TokenizaciÃ³n (texto â†’ tokens numÃ©ricos)
# ============================================================================
print("\n" + "="*80)
print("PASO 2: TokenizaciÃ³n - Dividir en 'Tokens'")
print("="*80)

print("""
â“ Â¿QuÃ© es un token?
   Un token es una unidad bÃ¡sica de texto que el modelo entiende.
   Puede ser:
   â€¢ Una palabra completa: "pasta" â†’ 1 token
   â€¢ Parte de una palabra: "running" â†’ "run" + "##ning" (2 tokens)
   â€¢ PuntuaciÃ³n: "!" â†’ 1 token
   â€¢ Espacio: puede ser parte de un token

ğŸ“ BAAI/bge-small-en-v1.5 usa WordPiece tokenizer:
   â€¢ Vocabulario de ~30,000 tokens
   â€¢ Descompone palabras en subpalabras
   â€¢ Convierte cada token a un ID numÃ©rico
""")

# SimulaciÃ³n del proceso de tokenizaciÃ³n
print(f"\nğŸ” TokenizaciÃ³n de '{text}':\n")
print("   Paso 2.1: Dividir en palabras")
words = text.split()
print(f"   Palabras: {words}\n")

print("   Paso 2.2: Convertir cada palabra a token IDs")
print("   (Simulado - el modelo real usa WordPiece)")
token_ids = {
    "quick": 2032,
    "pasta": 8459,
    "recipe": 7394,
}
print(f"   'quick' â†’ Token ID: {token_ids['quick']}")
print(f"   'pasta' â†’ Token ID: {token_ids['pasta']}")
print(f"   'recipe' â†’ Token ID: {token_ids['recipe']}\n")

print("   Resultado: [2032, 8459, 7394] (3 tokens)")
print("\n   ğŸ’¡ El texto ahora es una secuencia de nÃºmeros enteros")

# ============================================================================
# PASO 3: Embedding Table Lookup (token IDs â†’ vectores iniciales)
# ============================================================================
print("\n" + "="*80)
print("PASO 3: Embedding Table - De IDs a Vectores Iniciales")
print("="*80)

print("""
El modelo tiene una TABLA DE EMBEDDINGS:
   â€¢ Es una matriz gigante: [vocab_size Ã— embedding_dim]
   â€¢ Para BAAI/bge: [30,000 tokens Ã— 384 dimensiones]
   â€¢ Cada token tiene su propio vector de 384 nÃºmeros

ğŸ“Š Tabla de Embeddings (simplificado):
   
   Token ID â”‚ Embedding (384 dims)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   0        â”‚ [0.023, -0.145, 0.067, ..., 0.091]
   1        â”‚ [-0.089, 0.234, -0.012, ..., 0.156]
   ...      â”‚ ...
   2032     â”‚ [-0.109, -0.018, 0.017, ..., 0.003]  â† "quick"
   ...      â”‚ ...
   8459     â”‚ [0.234, -0.156, 0.089, ..., -0.045]  â† "pasta"
   ...      â”‚ ...
   7394     â”‚ [0.067, 0.123, -0.091, ..., 0.078]   â† "recipe"
   ...      â”‚ ...
   29999    â”‚ [0.145, -0.067, 0.234, ..., -0.023]

ğŸ” Lookup de cada token:
""")

# SimulaciÃ³n de embeddings iniciales
embedding_quick = np.random.randn(384) * 0.1
embedding_pasta = np.random.randn(384) * 0.1
embedding_recipe = np.random.randn(384) * 0.1

print(f"\n   Token 'quick' (ID: 2032)")
print(f"   â†’ Vector inicial: [{embedding_quick[0]:.3f}, {embedding_quick[1]:.3f}, ..., {embedding_quick[-1]:.3f}]")
print(f"      (384 nÃºmeros)\n")

print(f"   Token 'pasta' (ID: 8459)")
print(f"   â†’ Vector inicial: [{embedding_pasta[0]:.3f}, {embedding_pasta[1]:.3f}, ..., {embedding_pasta[-1]:.3f}]")
print(f"      (384 nÃºmeros)\n")

print(f"   Token 'recipe' (ID: 7394)")
print(f"   â†’ Vector inicial: [{embedding_recipe[0]:.3f}, {embedding_recipe[1]:.3f}, ..., {embedding_recipe[-1]:.3f}]")
print(f"      (384 nÃºmeros)\n")

print("   Tenemos ahora: 3 vectores de 384 dimensiones")
print("   Forma: [3 tokens Ã— 384 dims]\n")

print("   ğŸ’¡ Estos son embeddings ESTÃTICOS (no cambian entre queries)")

# ============================================================================
# PASO 4: Transformer Encoder (procesa contexto)
# ============================================================================
print("\n" + "="*80)
print("PASO 4: Transformer Encoder - Procesar Contexto")
print("="*80)

print("""
ğŸ§  AQUÃ ESTÃ LA MAGIA - El Transformer:

Los embeddings iniciales son solo el punto de partida.
El Transformer (arquitectura BERT-like) procesa estos vectores para:
   â€¢ Entender el CONTEXTO de cada palabra
   â€¢ Capturar relaciones entre palabras
   â€¢ Ajustar los vectores segÃºn el significado global

ğŸ“ Arquitectura del Transformer (BAAI/bge-small-en-v1.5):
   
   Input: [3 tokens Ã— 384 dims]
      â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TRANSFORMER ENCODER (12 capas)      â”‚
   â”‚                                     â”‚
   â”‚  Capa 1: Self-Attention +           â”‚
   â”‚          Feed-Forward               â”‚
   â”‚     â†“                               â”‚
   â”‚  Capa 2: Self-Attention +           â”‚
   â”‚          Feed-Forward               â”‚
   â”‚     â†“                               â”‚
   â”‚  ...                                â”‚
   â”‚     â†“                               â”‚
   â”‚  Capa 12: Self-Attention +          â”‚
   â”‚           Feed-Forward              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   Output: [3 tokens Ã— 384 dims]
      (ajustados segÃºn contexto)

ğŸ” Â¿QuÃ© hace SELF-ATTENTION?

Ejemplo con "quick pasta recipe":
   
   Palabra "pasta" mira a:
   â€¢ "quick" â†’ Â¿Es pasta rÃ¡pida? (peso: 0.6)
   â€¢ "pasta" â†’ SÃ­ mismo (peso: 0.3)
   â€¢ "recipe" â†’ Â¿Receta de pasta? (peso: 0.8)
   
   Y ajusta su vector segÃºn estos contextos:
   
   vector_pasta_nuevo = 
       0.6 * vector_quick +
       0.3 * vector_pasta +
       0.8 * vector_recipe
   
   ğŸ’¡ Ahora "pasta" tiene informaciÃ³n de TODO el contexto

Este proceso se repite 12 veces (12 capas).
Cada capa captura relaciones mÃ¡s abstractas.

ğŸ¯ Resultado despuÃ©s del Transformer:
   â€¢ Vector de "quick" estÃ¡ ajustado por "pasta" y "recipe"
   â€¢ Vector de "pasta" estÃ¡ ajustado por "quick" y "recipe"
   â€¢ Vector de "recipe" estÃ¡ ajustado por "quick" y "pasta"
   
   Cada vector ahora REPRESENTA EL CONTEXTO COMPLETO
""")

# ============================================================================
# PASO 5: Pooling (mÃºltiples vectores â†’ un solo vector)
# ============================================================================
print("\n" + "="*80)
print("PASO 5: Pooling - Combinar en UN Solo Vector")
print("="*80)

print("""
ğŸ¯ Problema: Tenemos 3 vectores (uno por token), pero queremos 1 solo.

ğŸ’¡ SoluciÃ³n: POOLING (combinar vectores)

Estrategias de pooling:

A) MEAN POOLING (promedio):
   vector_final = (vector_quick + vector_pasta + vector_recipe) / 3
   
   âœ… MÃ¡s usado en embeddings de texto
   âœ… Captura informaciÃ³n de todos los tokens
   âœ… BAAI/bge-small-en-v1.5 usa mean pooling

B) CLS TOKEN (primer token especial):
   vector_final = vector_[CLS]
   
   âœ… Usado en BERT original
   âŒ Ignora informaciÃ³n de otros tokens

C) MAX POOLING:
   Para cada dimensiÃ³n, toma el valor mÃ¡ximo de todos los tokens
   
   âŒ Menos comÃºn para embeddings

ğŸ“Š Proceso de Mean Pooling:
""")

print("\n   Vectores despuÃ©s del Transformer:")
print(f"   'quick':  [{embedding_quick[0]:.3f}, {embedding_quick[1]:.3f}, ..., {embedding_quick[-1]:.3f}]")
print(f"   'pasta':  [{embedding_pasta[0]:.3f}, {embedding_pasta[1]:.3f}, ..., {embedding_pasta[-1]:.3f}]")
print(f"   'recipe': [{embedding_recipe[0]:.3f}, {embedding_recipe[1]:.3f}, ..., {embedding_recipe[-1]:.3f}]\n")

# Simular mean pooling
final_embedding = (embedding_quick + embedding_pasta + embedding_recipe) / 3

print("   Mean Pooling (promedio por dimensiÃ³n):")
print(f"   Dim 0: ({embedding_quick[0]:.3f} + {embedding_pasta[0]:.3f} + {embedding_recipe[0]:.3f}) / 3 = {final_embedding[0]:.3f}")
print(f"   Dim 1: ({embedding_quick[1]:.3f} + {embedding_pasta[1]:.3f} + {embedding_recipe[1]:.3f}) / 3 = {final_embedding[1]:.3f}")
print("   ...")
print(f"   Dim 383: ({embedding_quick[-1]:.3f} + {embedding_pasta[-1]:.3f} + {embedding_recipe[-1]:.3f}) / 3 = {final_embedding[-1]:.3f}\n")

print(f"   Vector final: [{final_embedding[0]:.3f}, {final_embedding[1]:.3f}, ..., {final_embedding[-1]:.3f}]")
print(f"   Dimensiones: {final_embedding.shape[0]}\n")

print("   ğŸ’¡ Este vector REPRESENTA el significado completo de 'quick pasta recipe'")

# ============================================================================
# PASO 6: NormalizaciÃ³n (opcional pero importante)
# ============================================================================
print("\n" + "="*80)
print("PASO 6: NormalizaciÃ³n - Estandarizar el Vector")
print("="*80)

print("""
ğŸ¯ NormalizaciÃ³n L2 (unit norm):
   Escalar el vector para que tenga longitud = 1.0
   
   FÃ³rmula:
   vector_normalizado = vector / ||vector||
   
   Donde ||vector|| = sqrt(sum(xÂ² for x in vector))

ğŸ“Š Â¿Por quÃ© normalizar?
   â€¢ Distancia coseno se simplifica a dot product
   â€¢ Todos los vectores tienen la misma "magnitud"
   â€¢ Solo importa la DIRECCIÃ“N, no la longitud
""")

norm = np.linalg.norm(final_embedding)
normalized_embedding = final_embedding / norm

print(f"\n   Vector antes de normalizar:")
print(f"   Norma (longitud): {norm:.3f}")
print(f"   Vector: [{final_embedding[0]:.3f}, {final_embedding[1]:.3f}, ..., {final_embedding[-1]:.3f}]\n")

print(f"   Vector despuÃ©s de normalizar:")
print(f"   Norma (longitud): {np.linalg.norm(normalized_embedding):.3f} (siempre 1.0)")
print(f"   Vector: [{normalized_embedding[0]:.3f}, {normalized_embedding[1]:.3f}, ..., {normalized_embedding[-1]:.3f}]\n")

print("   ğŸ’¡ El vector normalizado es el EMBEDDING FINAL")

# ============================================================================
# PASO 7: DemostraciÃ³n REAL con el modelo
# ============================================================================
print("\n" + "="*80)
print("PASO 7: DemostraciÃ³n REAL - Todo el Proceso")
print("="*80)

print(f"\nğŸš€ Usando el modelo REAL: BAAI/bge-small-en-v1.5\n")

model = TextEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Generar embedding real
texts = ["quick pasta recipe", "fast spaghetti dish", "chocolate cake dessert"]

print("   Generando embeddings...\n")
embeddings_real = list(model.embed(texts))

for text, emb in zip(texts, embeddings_real):
    print(f"   '{text}'")
    print(f"   â†’ Vector: [{emb[0]:.3f}, {emb[1]:.3f}, {emb[2]:.3f}, ..., {emb[-1]:.3f}]")
    print(f"   â†’ Dimensiones: {len(emb)}")
    print(f"   â†’ Norma: {np.linalg.norm(emb):.3f}")
    print()

# ============================================================================
# PASO 8: Â¿QuÃ© representa cada dimensiÃ³n?
# ============================================================================
print("\n" + "="*80)
print("PASO 8: Â¿QuÃ© Representa Cada DimensiÃ³n?")
print("="*80)

print("""
â“ Pregunta comÃºn: Â¿QuÃ© significa cada uno de los 384 nÃºmeros?

ğŸ’¡ Respuesta: NO tienen un significado directo interpretable.

ğŸ“Š Cada dimensiÃ³n es una CARACTERÃSTICA LATENTE:
   â€¢ NO es "velocidad" o "sabor" o "dificultad"
   â€¢ Son combinaciones abstractas aprendidas durante el entrenamiento
   â€¢ El modelo descubriÃ³ que estas 384 dimensiones son Ã³ptimas
     para capturar relaciones semÃ¡nticas

ğŸ¯ AnalogÃ­a:
   Es como los colores RGB:
   â€¢ (255, 0, 0) = rojo
   â€¢ Pero el "0" del verde no significa "ausencia de verde"
   â€¢ Es solo una REPRESENTACIÃ“N numÃ©rica
   
   En embeddings:
   â€¢ [0.123, -0.456, ...] = "quick pasta recipe"
   â€¢ Pero 0.123 en dim 0 no significa algo especÃ­fico
   â€¢ Es solo una REPRESENTACIÃ“N aprendida

ğŸ”¬ Lo que SÃ sabemos:
   â€¢ Vectores cercanos = significados similares
   â€¢ DirecciÃ³n del vector = tipo de concepto
   â€¢ Magnitud (antes de normalizar) = quÃ© tan "fuerte" es el concepto
   
ğŸ§ª Ejemplo de "direcciones" en el espacio vectorial:
   
   vector("quick") - vector("slow") â‰ˆ vector("fast") - vector("sluggish")
   
   Existe una "direcciÃ³n" para el concepto de "velocidad"
   pero NO es una dimensiÃ³n especÃ­fica, es una COMBINACIÃ“N de todas
""")

# ============================================================================
# RESUMEN COMPLETO
# ============================================================================
print("\n" + "="*80)
print("ğŸ“ RESUMEN: Texto â†’ Vector de 384 Dimensiones")
print("="*80)

print("""
PROCESO COMPLETO:

1ï¸âƒ£  TOKENIZACIÃ“N
    "quick pasta recipe" 
    â†’ ["quick", "pasta", "recipe"]
    â†’ [2032, 8459, 7394]

2ï¸âƒ£  EMBEDDING TABLE LOOKUP
    [2032, 8459, 7394]
    â†’ [vector_quick, vector_pasta, vector_recipe]
    â†’ Matriz [3 Ã— 384]

3ï¸âƒ£  TRANSFORMER ENCODER (12 capas)
    â€¢ Self-Attention: cada token mira a todos los demÃ¡s
    â€¢ Feed-Forward: procesamiento no-lineal
    â€¢ Repite 12 veces
    â†’ Vectores ajustados por CONTEXTO

4ï¸âƒ£  POOLING (Mean Pooling)
    [3 vectores Ã— 384 dims]
    â†’ Promedio de todos los tokens
    â†’ [1 vector Ã— 384 dims]

5ï¸âƒ£  NORMALIZACIÃ“N
    Vector / ||vector||
    â†’ Norma = 1.0
    â†’ EMBEDDING FINAL

ğŸ¯ RESULTADO:
    "quick pasta recipe" 
    â†’ [-0.109, -0.018, 0.017, ..., 0.003]
    â†’ 384 nÃºmeros que REPRESENTAN el significado completo

âš¡ VELOCIDAD:
    â€¢ Todo este proceso: ~10-50ms en CPU
    â€¢ En GPU: ~1-5ms
    â€¢ El modelo estÃ¡ OPTIMIZADO para ser rÃ¡pido

ğŸ’¾ TAMAÃ‘O DEL MODELO:
    â€¢ BAAI/bge-small-en-v1.5: ~134 MB
    â€¢ Parametros: ~33 millones
    â€¢ Compacto y eficiente

ğŸš€ USO EN BÃšSQUEDA:
    1. Usuario escribe query â†’ Embedding (50ms)
    2. Compara con todos los chunks en BD â†’ pgvector (200ms)
    3. Retorna top K resultados
    
    Total: ~250ms para buscar en miles de recetas
""")

print("\n" + "="*80)
print("âœ… ExplicaciÃ³n completada!")
print("="*80)

