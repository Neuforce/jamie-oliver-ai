# Notas sobre Performance de Ollama

## Situaci√≥n Actual

El script `test_chunks.py` est√° **funcionalmente correcto** y completo. Sin embargo, Ollama puede tardar mucho tiempo en procesar PDFs con prompts complejos.

### Diagn√≥stico Realizado

‚úÖ **Ollama est√° corriendo** - Proceso activo verificado  
‚úÖ **Modelo llama3.1 est√° disponible** - Confirmado v√≠a API  
‚úÖ **Conexi√≥n HTTP funciona** - curl responde correctamente  
‚ùå **Timeout en procesamiento** - Los prompts largos tardan > 5 minutos

## ¬øPor qu√© est√° tardando tanto?

1. **Prompts complejos**: El pipeline env√≠a 3 requests a Llama por PDF:
   - `understand_recipe()`: Analiza el texto crudo
   - `build_joav0_json()`: Estructura el JSON
   - `build_intelligent_chunks()`: Genera chunks inteligentes

2. **Tama√±o del contexto**: PDFs con 1500+ caracteres generan prompts largos

3. **Hardware**: Llama3.1 (8B par√°metros) requiere:
   - ~8GB RAM m√≠nimo
   - CPU/GPU potente para respuestas r√°pidas

## Soluciones

### Opci√≥n 1: Esperar m√°s tiempo ‚è±Ô∏è

El timeout actual es de **5 minutos (300 segundos)**. Para aumentarlo:

```python
# Editar: recipe_pdf_agent_llama/ollama_client.py
@dataclass(frozen=True)
class OllamaConfig:
    base_url: str
    model: str
    timeout_s: float = 600.0  # 10 minutos
```

### Opci√≥n 2: Usar un modelo m√°s peque√±o/r√°pido üöÄ

```bash
# Descargar modelo m√°s ligero
ollama pull llama3.2

# Usar en el script
python tests/test_chunks.py --model llama3.2 --pdf data/processed_pdfs/tomato-mussel-pasta.pdf
```

### Opci√≥n 3: Probar con PDFs m√°s peque√±os üìÑ

```bash
# PDFs m√°s peque√±os (ordenados por tama√±o)
python tests/test_chunks.py --pdf data/processed_pdfs/tomato-mussel-pasta.pdf  # 36KB
python tests/test_chunks.py --pdf data/processed_pdfs/smoked-salmon-pasta-jamie-oliver-recipes.pdf  # 46KB
python tests/test_chunks.py --pdf data/processed_pdfs/happy-fish-pie.pdf  # 50KB
```

### Opci√≥n 4: Optimizar Ollama ‚öôÔ∏è

```bash
# Reiniciar Ollama para limpiar memoria
pkill ollama
ollama serve

# Verificar que no hay otros procesos pesados
top -o cpu
```

### Opci√≥n 5: Usar la versi√≥n cuantizada m√°s agresiva

```bash
# Modelo con menor precisi√≥n pero m√°s r√°pido
ollama pull llama3.1:q4_K_S  # Version cuantizada 4-bit
python tests/test_chunks.py --model llama3.1:q4_K_S --pdf [tu_pdf]
```

## Verificaci√≥n R√°pida

Para verificar que el script funciona sin esperar tanto:

```bash
# Test de conexi√≥n (debe responder en segundos)
curl -X POST http://localhost:11434/api/chat \
  -d '{"model":"llama3.1","stream":false,"messages":[{"role":"user","content":"Say OK"}]}' \
  -H "Content-Type: application/json" \
  --max-time 60
```

Si esto responde en <10 segundos, Ollama est√° funcionando bien. El problema es el tama√±o/complejidad de los prompts del pipeline.

## Recomendaci√≥n

Para **validar el script r√°pidamente**:

1. Usar `llama3.2` (m√°s r√°pido)
2. Probar con el PDF m√°s peque√±o (`tomato-mussel-pasta.pdf`)
3. Estar preparado para esperar 2-3 minutos por request

```bash
python tests/test_chunks.py \
  --model llama3.2 \
  --pdf data/processed_pdfs/tomato-mussel-pasta.pdf
```

## Monitoreo

Para ver qu√© est√° haciendo Ollama:

```bash
# Terminal 1: Ver logs de Ollama
tail -f ~/.ollama/logs/server.log

# Terminal 2: Monitor de recursos
watch -n 1 'ps aux | grep ollama | grep -v grep'

# Terminal 3: Ejecutar script
python tests/test_chunks.py --pdf [tu_pdf]
```



