import asyncio
import json

from typing import List
from ccai.core.logger import configure_logger
from ccai.core.brain.base import BaseBrain
from ccai.core.function_manager.base import BaseFunctionManager
from ccai.core.llm.base import BaseLLM, ChunkResponse, FunctionCallResponse
from ccai.core.memory.base import BaseChatMemory
from ccai.core.messages.base import (
    UserMessage,
    ToolMessage,
    ToolCall,
    Function,
)
from ccai.core.tracing import observe_voice_assistant, observe_function_call, tracer
#test

logger = configure_logger(__name__)


class SimpleBrain(BaseBrain):
    """
    A brain that processes user messages using a language model (LLM),
    manages chat memory, and handles function (tool) calls.
    """

    def __init__(
        self,
        llm: BaseLLM,
        chat_memory: BaseChatMemory,
        function_manager: BaseFunctionManager,
    ):
        """
        Initialize the SimpleBrain.

        Args:
            llm (BaseLLM): The language model to use for generating responses.
            chat_memory (BaseChatMemory): The chat memory for storing conversation history.
            function_manager (BaseFunctionManager): The manager for executing registered functions.
        """
        self.llm = llm
        self.chat_memory = chat_memory
        self.function_manager = function_manager

    @observe_voice_assistant("brain_message_processing")
    async def process(self, message: UserMessage):
        """
        Process a user message by interacting with the LLM and handling any function calls.

        Args:
            message (UserMessage): The user message to process.

        Yields:
            LLMResponse: Responses generated by the LLM, including text chunks and function call events.
        """
        # Add the user message to the chat memory
        self.chat_memory.add_user_message(message.content)
        logger.debug(f"User: {message.content}")

        # Implement a state machine approach instead of a while loop
        has_function_calls = True
        iteration_count = 0
        max_iterations = 5  # Safety limit to prevent infinite loops

        while has_function_calls and iteration_count < max_iterations:
            iteration_count += 1
            # Yield control back to the event loop occasionally
            await asyncio.sleep(0)

            # Prepare the conversation history and available functions
            conversation_history = self.chat_memory.get_messages()
            registered_functions = self.function_manager.get_registered_functions()

            # Invoke the LLM to get responses
            llm_response = self.llm.invoke(
                messages=conversation_history,
                function_registry=registered_functions,
            )

            # Initialize variables for processing the LLM response
            assistant_content = ""
            tool_calls: List[ToolCall] = []
            function_call_tasks = []

            # Process events from the LLM response
            async for event in llm_response:
                # Periodically yield control back to the event loop during long streams
                if len(tool_calls) % 5 == 0 and len(tool_calls) > 0:
                    await asyncio.sleep(0)

                if isinstance(event, ChunkResponse):
                    # Accumulate assistant's content and yield the chunk
                    assistant_content += event.content
                    yield event  # Yield the event to the caller
                elif isinstance(event, FunctionCallResponse):
                    logger.debug(
                        f"Received FunctionCallResponse [{event.tool_call_id}]: {event.function_name} | {event.arguments}"
                    )
                    # Serialize arguments to JSON string
                    serialized_arguments = json.dumps(event.arguments)

                    # Create a ToolCall for the function call event
                    tool_call = ToolCall(
                        id=event.tool_call_id,
                        function=Function(
                            name=event.function_name,
                            arguments=serialized_arguments,
                        ),
                    )
                    tool_calls.append(tool_call)

                    # Schedule the function call for execution
                    function_task = self._process_function_call(event)
                    function_call_tasks.append(function_task)
                else:
                    logger.warning(f"Unhandled event type: {type(event)}")

            # Set flag for next iteration based on whether we have function calls
            has_function_calls = bool(tool_calls)

            if tool_calls:
                # Add the assistant's message with function calls to the chat memory
                self.chat_memory.add_assistant_message(
                    content=assistant_content if assistant_content else None,
                    tool_calls=tool_calls,
                )
                logger.debug(
                    f"Assistant message with function calls added to chat memory."
                )

                # Execute all function calls concurrently and gather their results
                try:
                    # Use a timeout to prevent hanging on function calls
                    tool_responses = await asyncio.gather(*function_call_tasks)
                    logger.warning(tool_responses)
                except asyncio.TimeoutError:
                    logger.error("Function execution timed out")
                    # Break the loop if functions time out
                    break
                except Exception as e:
                    logger.error(f"Error during function execution: {e}", exc_info=True)
                    # Break the loop on error
                    break

                # Add tool responses to the chat memory
                for tool_response in tool_responses:
                    self.chat_memory.add_tool_message(
                        content=(
                            tool_response.content if tool_response.content else None
                        ),
                        tool_call_id=tool_response.tool_call_id,
                        function_name=tool_response.function_name,
                    )
                    logger.debug(
                        f"Tool response added for tool_call_id: {tool_response.tool_call_id}"
                    )
            else:
                # No function calls; add the assistant's message to the chat memory and exit the loop
                self.chat_memory.add_assistant_message(content=assistant_content)
                logger.debug(f"Assistant: {assistant_content}")
                logger.debug(
                    f"Assistant message added to chat memory without function calls."
                )

        if iteration_count >= max_iterations:
            logger.warning(
                f"Reached maximum iterations ({max_iterations}) in brain processing"
            )

    @observe_function_call("dynamic_function")
    async def _process_function_call(
        self, function_call: FunctionCallResponse
    ) -> ToolMessage:
        """
        Execute a function call and return its response as a ToolMessage.

        Args:
            function_call (FunctionCallResponse): The function call event from the LLM.

        Returns:
            ToolMessage: The response from the function execution.
        """
        try:
            # Deserialize arguments from JSON string to dictionary
            arguments = json.loads(json.dumps(function_call.arguments))

            # Execute the function using the function manager
            response = await self.function_manager.execute_function(
                function_name=function_call.function_name,
                arguments=arguments,
            )
            logger.debug(
                f"Function '{function_call.function_name}' executed successfully."
            )
        except Exception as e:
            # Handle exceptions during function execution
            error_message = (
                f"Error executing function '{function_call.function_name}': {str(e)}"
            )
            logger.error(error_message, exc_info=True)
            response = (
                error_message  # You might choose to return a generic error message
            )

        # Return the function's response as a ToolMessage
        logger.info(f"function_call.tool_call_id: {function_call.tool_call_id}")
        return ToolMessage(
            content=response,
            tool_call_id=function_call.tool_call_id,
            function_name=function_call.function_name,
        )
